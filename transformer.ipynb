{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b610503-465f-44ff-9523-bfbe05858495",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Understanding and Implementing Transformers from Scratch\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde58b0-05e9-43f0-a304-1f9992455adc",
   "metadata": {},
   "source": [
    "The Transformer model was introduced in the paper \"Attention Is All You Need.\" The standard (vanilla) Transformer consists of two main components: an encoder and a decoder. In this notebook, I will explain the model step by step and build it from scratch using PyTorch.\n",
    "\n",
    "First, I will introduce the encoder, followed by the decoder. To enhance understanding, I will also illustrate the model with a simple example. Below is the architecture of the Transformer model (adapted from the original paper). This diagram provides a high-level overview of the model's structure. I will explore each component in detail, discussing both the conceptual understanding and the step-by-step implementation, based on my perspective as an applied mathematician."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9e90e-be21-47af-a421-e45313c3531f",
   "metadata": {},
   "source": [
    "<img src=\"transformer.png\" alt=\"Transformer Model\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c682db1-e79c-4df2-abbb-a1dfcbf6730f",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7777ee-aeaa-47de-96b1-df9913bcaccd",
   "metadata": {},
   "source": [
    "First, let's examine the encoder component. I will explain the process from the inputs to the input embedding block (**see the right figure**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d55d27-16a5-49d2-a4fd-f9e8bf57af9e",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"encoder.png\" alt=\"Encoder\" width=\"45%\">\n",
    "    <img src=\"encoder_input_embedding.png\" alt=\"Input Embedding\" width=\"45%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94572d0-bceb-42b6-8026-4a778fd78f37",
   "metadata": {},
   "source": [
    "### Input Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaaa03b-33b1-4e07-9cb1-66d727fa316f",
   "metadata": {},
   "source": [
    "Suppose we need to translate from English to French, such as:\n",
    "* \"I love you\" ‚Üí \"Je t‚Äôaime\"\n",
    "* \"I like you\" ‚Üí \"Je t‚Äôaime bien\"\n",
    "The sentences \"I love you\" and \"I like you\" should be fed into the encoder as inputs. However, before doing so, we must first transform the text into numerical representations. For example, we can map words to integers as follows: {'I': 1, 'love': 2, 'you': 3,'like': 4}.\n",
    "\n",
    "Using this mapping:\n",
    "* \"I love you\" becomes [1, 2, 3] instead of the raw text.\n",
    "* \"I like you\" becomes [1, 4, 3].\n",
    "  \n",
    "However, this method has a major limitation: it treats all words as equally distant from each other. The model has no way of knowing that \"love\" and \"like\" are semantically related, while \"I\" and \"you\" are not.\n",
    "\n",
    "To address this, we use embeddings, which map words into a continuous, high-dimensional space where semantically similar words have closer distances. Instead of treating words as mere integers, embeddings assign each word a learned vector representation, such as:\n",
    "* \"love\"‚Üí[0.21,0.87,‚àí0.32,...]\n",
    "* \"like\"‚Üí[0.22,0.85,‚àí0.30,...]\n",
    "Since \"love\" and \"like\" are close in meaning, their vector representations are also close in space. This structure allows the model to understand word relationships, which is crucial for capturing meaning in natural language processing.\n",
    "\n",
    "Moreover, because Transformers perform mathematical operations (such as self-attention) on input data, they require continuous values. Embeddings enable the model to process and learn from text effectively.\n",
    "\n",
    "This brings us to the concept of Input Embedding, where words are transformed from discrete integer IDs into meaningful high-dimensional representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995e3ad0-a233-42f2-ade4-cbf7f136be0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([1, 3, 6])\n",
      "Embedded Output:\n",
      " tensor([[[-0.6899, -0.7213, -1.3537, -1.0998,  0.2229,  0.3120],\n",
      "         [ 0.1424,  1.2600,  0.7236, -1.4959,  1.2815,  0.4239],\n",
      "         [ 1.9486,  0.4626,  1.1677, -1.0220, -0.9651,  0.6257]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# Define the embedding layer \n",
    "embedding = nn.Embedding(num_embeddings=40, embedding_dim=6)  # num_embeddings(the total number of unique words and special tokens).\n",
    "\n",
    "# Example input tensor (word indices)\n",
    "input_tensor = torch.tensor([[1, 2, 3]])\n",
    "\n",
    "# Get the embedded output\n",
    "embedded_output = embedding(input_tensor)\n",
    "\n",
    "# Print shape and output\n",
    "print(\"Shape of output:\", embedded_output.shape)  \n",
    "print(\"Embedded Output:\\n\", embedded_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ce1aa-f751-4621-aa62-af2fd52fa36a",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd5b4c-25d9-406d-b93f-2a2e4d188e9c",
   "metadata": {},
   "source": [
    "<img src=\"positional_encoding.png\" alt=\"Positional encoding\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b73ae-0f93-4cc6-9a43-e2e886c86a5a",
   "metadata": {},
   "source": [
    "Since the Transformer model's self-attention mechanism does not inherently capture the order of words in a sequence‚Äîunlike RNNs, which process inputs sequentially‚Äîwe need to explicitly provide positional information along with the input embeddings. This ensures that the model can differentiate word positions and retain the correct order within a sentence.\n",
    "\n",
    "For example, in the sentence \"I love you\", after applying self-attention, the model may treat the words (\"I\", \"love\", \"you\") in any order, as self-attention alone does not encode positional dependencies. To address this, we introduce positional encoding, which helps the model incorporate order information and correctly interpret the sequence.\n",
    "\n",
    "The positional encoding should have the properties:\n",
    "* Unique encoding for each position across the entire corpus: The position values should be consistent across all sentences in the dataset, ensuring that the model can correctly differentiate word positions regardless of sentence length.\n",
    "\n",
    "* A predictable relationship between positions: If we know the position p of one word, we should be able to easily determine the position p + k of another word. This helps the model recognize positional patterns more effectively.\n",
    "\n",
    "A natural way to define position is by using the index of each word in the sentence (e.g., 1, 2, 3, ...). However, if a sentence is very long, such as 200 words, the positional values would range from (1, 2, 3, ..., 200).\n",
    "\n",
    "If we add the positional encoding to the input embedding of a word (e.g., \"love\") in such a sentence, it would result in: Positional Encoding + Input Embedding --> [200+0.21,200+0.87,200+‚àí0.32,...]. These large positional values could dominate the original embedding, making it harder for the model to retain the actual meaning of the word \"love\".\n",
    "\n",
    "**Sinusoidal Encoding:**\n",
    "\n",
    "One way to define positional encoding that satisfies the above conditions is by using sinusoidal functions. Suppose $d_{model}$  is the dimension of the model (i.e., the dimension of both the input embeddings and the positional encoding). The positional encoding is defined as:\n",
    "* For even indices ($ùëñ$ is even):\n",
    "  $$Positional \\ Encoding(pos,i) = sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "* For odd indices ($ùëñ$ is odd):\n",
    "  $$Positional\\ Encoding(pos,i) = cos(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "\n",
    "**For any two distinct positions $p$ and $j$, the vectors generated by the sinusoidal positional encoding function are not identical for all $i$ from $0$ to $d_{\\text{model}} - 1$. A contradiction can be found when considering $i = 0$.** \n",
    "\n",
    "**Proof:**  \n",
    "Let $p$ and $j$ be two distinct positions such that $p \\neq j$. Suppose, for contradiction, that:\n",
    "\n",
    "$\\text{Positional Encoding}(p,0) = \\text{Positional Encoding}(j,0)$\n",
    "\n",
    "From the definition of positional encoding:\n",
    "\n",
    "$\\sin\\left(\\frac{p}{10000^{\\frac{2 \\cdot 0}{d_{\\text{model}}}}} \\right) = \\sin\\left(\\frac{j}{10000^{\\frac{2 \\cdot 0}{d_{\\text{model}}}}} \\right)$\n",
    "\n",
    "Since the sine function is periodic, this implies:\n",
    "\n",
    "$\\frac{p}{10000^{\\frac{2 \\cdot 0}{d_{\\text{model}}}}} = \\frac{j}{10000^{\\frac{2 \\cdot 0}{d_{\\text{model}}}}} + 2k\\pi$\n",
    "\n",
    "for some integer \\( k \\). Simplifying, we obtain:\n",
    "$p - j = 2k\\pi$\n",
    "\n",
    "Since $p$ and $j$ are integers, the left-hand side of the equation is necessarily an integer. However, the right-hand side, $2k\\pi$, is an irrational number unless $k = 0$. This is a contradiction, as no integer can be equal to a nonzero irrational number.\n",
    "\n",
    "Thus, the assumption that positional encoding vectors could be identical for distinct positions is false. Therefore, for any two distinct positions $p$ and $j$, their positional encoding vectors must differ for at least one dimension $i$.   (more general proof can be comparing the transcendental number and algebraic number)\n",
    "\n",
    "\n",
    "Now let consider the second property of Sinusoidal Encoding\n",
    "* A predictable relationship between positions: If we know the position p of one word, we should be able to easily determine the position p + k of another word. This helps the model recognize positional patterns more effectively.\n",
    "\n",
    "Transformer positional encoding uses both sine and cosine functions to provide a unique representation for each position in a sequence. The sinusoidal encoding ensures that the relative positions between tokens are encoded in a way that preserves distance information. Specifically, for any two positions\n",
    "$p$ and $j$ with the same relative difference $h=j-p$, the difference between their positional encodings remains the same regardless of their absolute positions. This allows the self-attention mechanism to generalize to sequences of different lengths without requiring explicit learned position embeddings.\n",
    "\n",
    "**Proof:** \n",
    "Let's set $\\omega_i = \\frac{1}{10000^{2i/d_{model}}}$, then we have \n",
    "$$\n",
    "\\left[\n",
    "\\begin{aligned}\n",
    "\\sin(\\omega_i j)  \\\\\n",
    "\\cos(\\omega_i j) \n",
    "\\end{aligned}\n",
    "\\right] = \\left[\n",
    "\\begin{aligned}\n",
    "\\sin(\\omega_i (p+h)) \\\\\n",
    "\\cos(\\omega_i (p+h))\n",
    "\\end{aligned}\n",
    "\\right] = \\left[\n",
    "\\begin{aligned}\n",
    "\\cos(\\omega_i h) \\  \\sin(\\omega_i h) \\\\\n",
    "-\\sin(\\omega_i h)\\ \\cos(\\omega_i h)\n",
    "\\end{aligned}\n",
    "\\right]\\cdot \\left[\n",
    "\\begin{aligned}\n",
    "\\sin(\\omega_i p)  \\\\\n",
    "\\cos(\\omega_i p) \n",
    "\\end{aligned}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This shows that when the difference between positions $p$ and $j$ in the positional index is the same, the corresponding transformation in the Sinusoidal Encoding remains unchanged:\n",
    "$$ \\left[\n",
    "\\begin{aligned}\n",
    "\\cos(\\omega_i h) \\  \\sin(\\omega_i h) \\\\\n",
    "-\\sin(\\omega_i h)\\ \\cos(\\omega_i h)\n",
    "\\end{aligned}\n",
    "\\right]\n",
    "$$\n",
    "This transformation depends only on the relative difference $h = j-p$ and is independent of the absolute values of $p$ and $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bdc790-6672-43a6-9775-bb8e3621d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        return x + self.pe[:, :x.size(1)]  # the model will consume the sum of the embedding and the position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8954cd06-ecbe-48e8-ade7-1ba1d0ad0f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.69  0.28 -1.35 -0.1   0.22  1.31]\n",
      "  [ 0.98  1.8   0.77 -0.5   1.28  1.42]\n",
      "  [ 2.86  0.05  1.26 -0.03 -0.96  1.63]]]\n",
      "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
      "         [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000]]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_model = 6 # which is consistent as the above example\n",
    "max_seq_length = 3 \n",
    "positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "position_plus_embedding = positional_encoding(embedded_output)\n",
    "print(position_plus_embedding.detach().numpy().round(2))\n",
    "print(position_plus_embedding - embedded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a24286-0edc-40da-b188-568e29071f9e",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b97f7a-39f5-4332-bc41-dce27243f506",
   "metadata": {},
   "source": [
    "<img src=\"mutihead_attention.png\" alt=\"Positional encoding\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c219bd-4ffd-4965-ad49-3400ec1c4f6e",
   "metadata": {},
   "source": [
    "#### Query, Key, and Value in Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61f7d7-ef62-49df-9a60-f37386fc8012",
   "metadata": {},
   "source": [
    "For this block, the input is the sum of the input embedding and the positional encoding. This combined input is then transformed for the multi-head attention block into:\n",
    "* Query: q\n",
    "* Key: k\n",
    "* Value: v\n",
    "\n",
    "There are many explanations for query, key, and value in attention mechanisms. Here‚Äôs how I understand it:\n",
    "\n",
    "Imagine you have a question and you know that the answer can be found in a book. When you open the book, you first scan through the table of contents, comparing the chapter titles with your question. Once you find keywords that closely match your question, you turn to the relevant section and read the details.\n",
    "\n",
    "In this analogy:\n",
    "* Your question is the query.\n",
    "* The chapter titles in the table of contents are the keys.\n",
    "* The detailed content inside each chapter is the value.\n",
    "\n",
    "When comparing your query with the keys, the closer the match, the more information you extract from that section. If there‚Äôs a strong match, you rely heavily on that content. If the match is weak, you gather less information from it.\n",
    "\n",
    "This is how attention mechanisms work‚Äîby weighing different pieces of information based on their relevance to the query. To have the variables q,k,v, we need transform the each input (each term) from the sum of input embedding and positional encoding: $q = xW_q$, $k = xW_k$, and $v = xW_v$. Below we show an example transforming inputs to query, key and value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c8849-b383-4e35-be53-611de9408f55",
   "metadata": {},
   "source": [
    "<img src=\"qkv.png\" alt=\"Positional encoding\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32c927c-f955-4409-8a25-4913ba047314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "         \n",
    "        Q = self.W_q(Q)\n",
    "         \n",
    "        K = self.W_k(K)\n",
    "         \n",
    "        V = self.W_v(V)\n",
    "\n",
    "        return Q,K,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad42de6f-ea34-47db-9c4c-e421affe33c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5244,  0.1807,  0.5842, -0.7224, -0.3677,  0.6021],\n",
      "         [ 1.1129,  0.5850,  0.5045, -0.0854, -0.6448, -0.7773],\n",
      "         [ 0.8135,  1.2542, -0.1623,  1.6789, -0.0822, -0.6191]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.2834,  0.1222,  0.5653, -0.4237, -0.2857, -0.1194],\n",
      "         [-0.5558, -0.0443, -0.7159, -1.0623,  0.3462,  0.3581],\n",
      "         [ 0.4966, -0.2995, -0.6203, -1.0580,  0.3751,  1.6195]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.2462,  0.2801,  0.2107, -0.0255,  0.6793, -0.0926],\n",
      "         [ 0.3025,  0.0955,  0.5272, -0.3304,  0.4018,  0.3059],\n",
      "         [ 1.1790, -0.4415, -0.4340, -0.0909,  0.6732,  0.3955]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention = SelfAttention(d_model)\n",
    "Q,K,V = self_attention(position_plus_embedding,position_plus_embedding,position_plus_embedding)\n",
    "print(Q)\n",
    "print(K)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b772492-addf-494e-b6fd-6df81956065b",
   "metadata": {},
   "source": [
    "#### Scaled product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc70e1-b7f8-4291-ba31-ac1e110f2c90",
   "metadata": {},
   "source": [
    "<img src=\"self_attention.png\" alt=\"Positional encoding\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206612a-c55b-44d6-a49d-b08752ec709c",
   "metadata": {},
   "source": [
    "We continue with the example \"I love you\". For each word‚Äî\"I,\" \"love,\" and \"you\"‚Äîwe extract three properties: query, key, and value, as discussed above.\n",
    "\n",
    "Following the same process, when answering the query for \"I\", we first compute the similarity between its query vector $q_I$ and the keys from all words: \"I,\" \"love,\" and \"you\". Next, we apply a softmax function to the scaled dot product to prevent excessively large values:\n",
    "$$\\text{Attention weights}\\quad  w = \\text{softmax}\\Big(\\frac{q_I\\cdot k}{\\sqrt{d_{model}}}\\Big)$$\n",
    "Using these weights, we aggregate the answer (output) for the query from the values corresponding to all words in the sentence:\n",
    "$$\\text{'answer'} = w_1 v_I + w_2 v_{love} + w_3 v_{you}$$\n",
    "A similar process is applied to compute attention for \"love\" and \"you,\" as illustrated in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105161ca-0751-42bb-8872-3689d3281b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_product_attention(self, Q, K, V):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        \n",
    "        attention_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "         \n",
    "        Q = self.W_q(Q)\n",
    "         \n",
    "        K = self.W_k(K)\n",
    "         \n",
    "        V = self.W_v(V)\n",
    "             \n",
    "        attn_output = self.scaled_product_attention(Q, K, V)\n",
    "        \n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6907e9b3-d0ef-403d-87a1-75dae944ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.04  0.36 -0.39  0.61  0.07  0.94]\n",
      "  [-0.38  0.52 -0.41  0.45  0.22  0.97]\n",
      "  [-0.57  0.58 -0.42  0.38  0.29  0.96]]]\n"
     ]
    }
   ],
   "source": [
    "self_attention = SelfAttention(d_model)\n",
    "attn_output = self_attention(position_plus_embedding,position_plus_embedding,position_plus_embedding)\n",
    "print(attn_output.detach().numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bb7f9-1bd3-466a-89de-15eb47f975eb",
   "metadata": {},
   "source": [
    "This step is extend the SelfAttention model to MultiHeadAttention model. \n",
    "\n",
    "**Feature-Splitting Multi-Head Attention**\n",
    "\n",
    "One way is to split the input tensor into multiple attention heads. Imagine reading a complex sentence. If you focus on everything at once, it‚Äôs overwhelming. Instead, your brain processes different parts separately.\n",
    "* (A) Each Head Learns Different Attention Patterns\n",
    "* (B) More Expressive Representations\n",
    "* (C) Reducing Computational Cost\n",
    "\n",
    "For example, consider the following query matrix:\n",
    "$$\n",
    "\\text{query} =\n",
    "\\begin{bmatrix}\n",
    "  0.53 & 0.11 & -1.24 & -0.05 & 0.03 & 0.08 \\\\\n",
    "  0.45 & 0.17 & -1.32 & -0.04 & 0.25 & 0.04 \\\\\n",
    "  0.45 & 0.20 & -1.30 & -0.04 & 0.27 & 0.03\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we apply **Multi-Head Attention** with $num\\_heads = 2$, we split the query into **two heads**, each processing a portion of the feature space:\n",
    "\n",
    "$$\n",
    "\\text{split\\_query} =\n",
    "\\begin{bmatrix}\n",
    "  \\begin{bmatrix} 0.53 & 0.11 & -1.24 \\\\ 0.45 & 0.17 & -1.32 \\\\ 0.45 & 0.20 & -1.30 \\end{bmatrix},\n",
    "  \\begin{bmatrix} -0.05 & 0.03 & 0.08 \\\\ -0.04 & 0.25 & 0.04 \\\\ -0.04 & 0.27 & 0.03 \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Similarly, the key and value matrices are split in the same way. Each head independently computes scaled dot-product attention on its respective sub-query, allowing different attention heads to focus on different parts of the input.\n",
    "\n",
    "In essence, each head processes a sub-section of the query (and similarly, the key and value) before the results are combined to form the final attention output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40eb0cd8-674b-49a8-ab75-e1dac586fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class SplittingMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(SplittingMultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "        \n",
    "    # def split_heads(self, x):\n",
    "    #     batch_size, seq_length, d_model = x.size()\n",
    "    #     return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    # def combine_heads(self, x):\n",
    "    #     batch_size, _, seq_length, d_k = x.size()\n",
    "    #     return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "         \n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "         \n",
    "        K = self.split_heads(self.W_k(K))\n",
    "         \n",
    "        V = self.split_heads(self.W_v(V))\n",
    "         \n",
    "        \n",
    "        attn_output = self.scaled_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e39ff-dbe8-42da-9f28-1206df9d64da",
   "metadata": {},
   "source": [
    "**Independent-Head Attention**\n",
    "\n",
    "* Each head acts as an independent \"expert\" that learns to understand the full input from a different perspective. Instead of splitting the features across heads (like standard multi-head attention), each head sees the entire representation.\n",
    "* The heads \"think\" independently, process the full information, and then communicate their findings by concatenating their outputs.\n",
    "* A final projection layer blends their insights together, ensuring that all perspectives contribute to the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896a333a-a8bb-4a40-988d-8ee0d2f031c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class IndependentHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(IndependentHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Each head gets its own full-dimensional projections\n",
    "        self.W_q = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_heads)])\n",
    "        self.W_k = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_heads)])\n",
    "        self.W_v = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(num_heads)])\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model * num_heads, d_model)  # Adjust for concatenated heads\n",
    "\n",
    "    def scaled_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        head_outputs = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            Q_i = self.W_q[i](Q)  \n",
    "            K_i = self.W_k[i](K)  \n",
    "            V_i = self.W_v[i](V)  \n",
    "            \n",
    "            attn_output = self.scaled_product_attention(Q_i, K_i, V_i, mask)\n",
    "            head_outputs.append(attn_output)\n",
    "        \n",
    "        # Concatenate along feature dimension\n",
    "        multi_head_output = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        # Final projection back to d_model\n",
    "        output = self.W_o(multi_head_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cb000-3232-4e04-bd31-52b8423ea423",
   "metadata": {},
   "source": [
    "### Add & Norm and Feed Forward blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86565521-46cc-4be5-8033-9a8221814193",
   "metadata": {},
   "source": [
    "<img src=\"add_norm.png\" alt=\"Positional encoding\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fc00c-ad27-4be7-935b-e6ea4ddaa0dc",
   "metadata": {},
   "source": [
    "In a Transformer, the Add & Norm layer plays a crucial role in stabilizing training and improving information flow.\n",
    "\n",
    "* The Add (Residual Connection) simply sums the attention output(attn_output) with the input embeddings (position_plus_embedding). This helps gradients flow more easily through the network, preventing vanishing gradients and allowing deeper models to be trained effectively.\n",
    "* The Norm (Layer Normalization) step then normalizes the summed output to stabilize the distribution of features, making training more efficient.\n",
    "\n",
    "After the Add & Norm step, the output is passed through the Feed Forward Network (FFN), which consists of: \n",
    "* A linear transformation followed by a non-linearity (e.g., ReLU or GELU). \n",
    "* Another linear transformation to project back to the original dimension.\n",
    "\n",
    "Mathematically, the process can be written as:\n",
    "**Norm(Residual+Attn¬†Output) ---> Feed¬†Forward ---> Norm**\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62149ad3-36f1-461f-bd2b-e00e76155d33",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e298db-c9fa-43a6-b26f-50028bc3ee8c",
   "metadata": {},
   "source": [
    "<img src=\"decoder.png\" alt=\"Positional encoding\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01114746-f509-421b-9334-01a5549a9fee",
   "metadata": {},
   "source": [
    "The decoder is quite similar to the encoder, but with some key differences. Therefore, we will focus on two main components: the output embeddings and the masked multi-head attention. In the masked multi-head attention, we will specifically discuss the masking mechanism, as the self-attention mechanism itself remains the same as in the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b449d22-a35e-4769-9418-d58a6e28bd13",
   "metadata": {},
   "source": [
    "### Outputs and Output Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ba240-13cc-4d5d-842a-1f191e78c913",
   "metadata": {},
   "source": [
    "To illustrate this, let‚Äôs consider the English-to-French translation of \"I love you\" ---> \"Je t‚Äôaime\". We begin by tokenizing the words separately. For example, we can assign token IDs as follows:\n",
    "\n",
    "* English: `{\"I\": 1, \"love\": 2, \"you\": 3, \"<pad>\": 0, \"<sos>\": 4, \"<eos>\": 5}`\n",
    "* French: `{\"Je\": 1, \"t‚Äôaime\": 2, \"<pad>\": 0, \"<sos>\": 3, \"<eos>\": 4}`\n",
    "  \n",
    "When working with multiple samples of varying sentence lengths, the <pad> token (0) is used to standardize input sizes by padding shorter sentences. Additionally, <sos> (Start of Sequence) and <eos> (End of Sequence) are special tokens that play a crucial role in training and inference. \n",
    "\n",
    "To train the decoder effectively, we right-shift the target sequence. This means the decoder input starts with <sos> and excludes <eos>, ensuring the model learns to predict each token step by step without seeing future words.\n",
    "\n",
    "For this example, assume the maximum sentence length for both inputs and outputs, including the start-of-sequence and end-of-sentence tokens, is 5:\n",
    "* The full target: `[\"<sos>\",\"Je\", \"t‚Äôaime\", \"<eos>\", \"<pad>\", \"<pad>\"] ‚Üí [3, 1, 2, 4, 0]`\n",
    "* Target output: Target output: `[\"Je\", \"t‚Äôaime\", \"<eos>\", \"<pad>\", \"<pad>\"] ‚Üí [1, 2, 4, 0]`\n",
    "* Right-shifted decoder input: `[\"<sos>\", \"Je\", \"t‚Äôaime\", \"<eos>\", \"<pad>\"] ‚Üí [3, 1, 2, 4]`\n",
    "  \n",
    "This setup enables the model to generate the next word autoregressively, aligning training with real-world inference. Once we obtain the tokens, the next step is to compute the output embeddings and apply positional encoding, just as in the encoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e88b53-d0ca-43ea-b839-bfacb3251ffb",
   "metadata": {},
   "source": [
    "### Masked Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc4c79-600b-4351-84e7-49d7bf8e767e",
   "metadata": {},
   "source": [
    "The masking operation serves two purposes: it prevents the model from attending to padding tokens and ensures that the decoder cannot access future tokens during training. The padding mask applies to both the encoder and decoder to ignore padded values, while the no-peek mask is used in the decoder to restrict attention to only past and present tokens, enforcing autoregressive generation.\n",
    "\n",
    "<img src=\"mask.png\" alt=\"Positional encoding\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "175bc663-9d8c-45c7-9080-125b3ec1e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "Embedded Output:\n",
      " tensor([[[ 0.2077,  0.9226,  0.8388,  1.4238, -0.8683, -1.1170],\n",
      "         [-1.2872, -0.0714, -0.5810,  3.1085, -0.5321,  0.6713],\n",
      "         [ 0.4108,  0.1605, -1.6998,  1.3860, -1.2406,  0.5303],\n",
      "         [-0.2086,  0.0436,  1.0710, -0.0247,  0.5650,  0.1487],\n",
      "         [-1.6737, -0.0327,  0.6171,  0.4939, -1.3666, -0.0686]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Embedded Output:\n",
      " tensor([[[ 0.4108,  0.1605, -1.6998,  1.3860, -1.2406,  0.5303],\n",
      "         [ 0.2077,  0.9226,  0.8388,  1.4238, -0.8683, -1.1170],\n",
      "         [-1.2872, -0.0714, -0.5810,  3.1085, -0.5321,  0.6713],\n",
      "         [-0.2086,  0.0436,  1.0710, -0.0247,  0.5650,  0.1487],\n",
      "         [-1.6737, -0.0327,  0.6171,  0.4939, -1.3666, -0.0686]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 6\n",
    "num_heads = 2\n",
    "num_layers = 1\n",
    "d_ff = 4*d_model\n",
    " \n",
    "batch_size = 1\n",
    "dropout = 0.1\n",
    "num_epochs = 300\n",
    "\n",
    "\n",
    "max_seq_length = 5\n",
    "# Define the embedding layer \n",
    "embedding = nn.Embedding(num_embeddings=40, embedding_dim=d_model)  # num_embeddings(the total number of unique words and special tokens).\n",
    "\n",
    "# Example input tensor (word indices)\n",
    "src_tensor = torch.tensor([[1, 2, 3, 4, 0]])\n",
    "print(src_tensor.shape)\n",
    "tgt_tensor = torch.tensor([[3, 1, 2, 4, 0]])\n",
    "\n",
    "# Get the embedded output\n",
    "embedded_src = embedding(src_tensor)\n",
    "embedded_tgt = embedding(tgt_tensor[:,:])\n",
    " \n",
    "print(\"Embedded Output:\\n\", embedded_src) \n",
    "print(\"Embedded Output:\\n\", embedded_tgt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee4cce61-4cf3-4258-8f21-4b436df5d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        return x + self.pe[:, :x.size(1)]  # the model will consume the sum of the embedding and the position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f07e6b-4f1d-4028-a87a-d668b7c717fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Positional_Encoding = PositionalEncoding(d_model,max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7733e2b6-d8b5-45f4-986d-5d0671edb19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2077,  1.9226,  0.8388,  2.4238, -0.8683, -0.1170],\n",
      "         [-0.4457,  0.4689, -0.5346,  4.1074, -0.5299,  1.6713],\n",
      "         [ 1.3201, -0.2556, -1.6071,  2.3817, -1.2363,  1.5303],\n",
      "         [-0.0674, -0.9464,  1.2098,  0.9657,  0.5714,  1.1487],\n",
      "         [-2.4305, -0.6863,  0.8017,  1.4767, -1.3580,  0.9314]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ 0.4108,  1.1605, -1.6998,  2.3860, -1.2406,  1.5303],\n",
      "         [ 1.0492,  1.4629,  0.8851,  2.4227, -0.8661, -0.1170],\n",
      "         [-0.3779, -0.4876, -0.4883,  4.1042, -0.5278,  1.6712],\n",
      "         [-0.0674, -0.9464,  1.2098,  0.9657,  0.5714,  1.1487],\n",
      "         [-2.4305, -0.6863,  0.8017,  1.4767, -1.3580,  0.9314]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedded_src_pos = Positional_Encoding(embedded_src)\n",
    "embedded_tgt_pos = Positional_Encoding(embedded_tgt)\n",
    "print(embedded_src_pos)\n",
    "print(embedded_tgt_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e658631c-fd26-406d-9f7d-19b8dfc0b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src, tgt):\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, src_len)\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, tgt_len)\n",
    "    \n",
    "    seq_length = tgt.size(1)\n",
    "    nopeak_mask = torch.tril(torch.ones(seq_length, seq_length)).bool().to(tgt.device) \n",
    "    tgt_mask = tgt_mask & nopeak_mask  # Apply no-peak mask correctly\n",
    "    \n",
    "    return src_mask, tgt_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2eaaac1-8d10-4350-a714-048c429097f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 1, 2, 4, 0]])\n",
      "tensor([[[[False,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_tensor[:,:])\n",
    "tgt_mask = (tgt_tensor[:,:] != 0).unsqueeze(1).unsqueeze(2) \n",
    "# print(tgt_mask)\n",
    "\n",
    "seq_length = tgt_tensor[:,:].size(1)\n",
    "nopeak_mask = torch.tril(torch.ones(seq_length, seq_length)).bool()\n",
    "# print(nopeak_mask)\n",
    "\n",
    "tgt_mask = tgt_mask & nopeak_mask\n",
    "# print(tgt_mask)\n",
    "\n",
    "\n",
    "print(tgt_mask==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a9bb367-b224-4878-8ebd-1106ef5b448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask, tgt_mask =  generate_mask(src_tensor, tgt_tensor)\n",
    "d_k = d_model // num_heads\n",
    "def scaled_product_attention(Q, K, V, mask=None):\n",
    "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "    print(attn_scores)\n",
    "    attention_weights = torch.softmax(attn_scores, dim=-1)\n",
    "    print(attention_weights) \n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0b46001-5da1-4e7b-a009-88d92c99d440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 6.4003e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [ 6.1088e+00,  1.1922e+01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "          [ 2.9457e+00,  7.5901e+00,  8.0446e+00, -1.0000e+09, -1.0000e+09],\n",
      "          [ 5.1447e-01,  2.6113e+00,  9.0066e-01,  2.8535e+00, -1.0000e+09],\n",
      "          [ 2.0193e+00,  5.0083e+00,  1.3279e+00,  2.0226e+00, -1.0000e+09]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0030, 0.9970, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0037, 0.3868, 0.6094, 0.0000, 0.0000],\n",
      "          [0.0477, 0.3879, 0.0701, 0.4943, 0.0000],\n",
      "          [0.0447, 0.8881, 0.0224, 0.0449, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = scaled_product_attention(embedded_src_pos, embedded_src_pos, embedded_src_pos, mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d026fd5-cc21-4fd1-9c57-53d7b498c943",
   "metadata": {},
   "source": [
    "## Full Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f86d8ed5-d939-4b0c-a6d3-a053995d0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 10\n",
    "num_heads = 1\n",
    "num_layers = 1\n",
    "d_ff = 4*d_model\n",
    " \n",
    "batch_size = 20\n",
    "dropout = 0.1\n",
    "num_epochs = 1\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_product_attention(self, Q, K, V, mask=None):\n",
    "         \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "        \n",
    "    # def split_heads(self, x):\n",
    "    #     batch_size, seq_length, d_model = x.size()\n",
    "    #     return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    # def combine_heads(self, x):\n",
    "    #     batch_size, _, seq_length, d_k = x.size()\n",
    "    #     return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "         \n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "         \n",
    "        K = self.split_heads(self.W_k(K))\n",
    "         \n",
    "        V = self.split_heads(self.W_v(V))\n",
    "         \n",
    "        \n",
    "        attn_output = self.scaled_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "         \n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        \n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ddc86-5b81-418f-8af0-ace14dbbdb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "36e9b320-8f6e-43df-852d-106039a41df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # def generate_mask(self, src, tgt):\n",
    "    #     src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    #     tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        \n",
    "    #     seq_length = tgt.size(1)\n",
    "    #     nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        \n",
    "         \n",
    "          \n",
    "    #     tgt_mask = tgt_mask & nopeak_mask\n",
    "         \n",
    "    #     return src_mask, tgt_mask\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, src_len)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, tgt_len)\n",
    "        \n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = torch.tril(torch.ones(seq_length, seq_length)).bool().to(tgt.device) \n",
    "        tgt_mask = tgt_mask & nopeak_mask  # Apply no-peak mask correctly\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "         \n",
    "        \n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        \n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "efab1e60-eb2a-47d7-86e9-04ae1afae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate random sample data\n",
    "# src_data = torch.randint(1, src_vocab_size, (batch_size, max_seq_length))  # (batch_size, seq_length)\n",
    "# tgt_data = torch.randint(1, tgt_vocab_size, (batch_size, max_seq_length))  # (batch_size, seq_length)\n",
    "# print(src_data)\n",
    " \n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Define extended vocabulary for English (source) and French (target)\n",
    "source_vocab_extended = {\n",
    "    \"I\": 1, \"love\": 2, \"you\": 3, \"hello\": 4, \"world\": 5, \n",
    "    \"good\": 6, \"morning\": 7, \"night\": 8, \"cat\": 9, \"dog\": 10,\n",
    "    \"he\": 11, \"she\": 12, \"we\": 13, \"they\": 14, \"go\": 15,\n",
    "    \"eat\": 16, \"run\": 17, \"walk\": 18, \"happy\": 19, \"sad\": 20,\n",
    "    \"big\": 21, \"small\": 22, \"fast\": 23, \"slow\": 24, \"boy\": 25, \"girl\": 26,\n",
    "    \"play\": 27, \"read\": 28, \"write\": 29, \"car\": 30, \"house\": 31,\n",
    "    \"plays\": 32, \"reads\": 33, \"writes\": 34, \"walks\": 35, \"runs\": 36, 'are':37,\n",
    "    \"<pad>\": 0, \"<sos>\": 38, \"<eos>\": 39\n",
    "}\n",
    "\n",
    "target_vocab_extended = {\n",
    "    \"Je\": 1, \"t‚Äôaime\": 2, \"bonjour\": 3, \"monde\": 4, \"bon\": 5, \n",
    "    \"matin\": 6, \"nuit\": 7, \"chat\": 8, \"chien\": 9, \"il\": 10,\n",
    "    \"elle\": 11, \"nous\": 12, \"ils\": 13, \"va\": 14, \"mange\": 15,\n",
    "    \"court\": 16, \"marche\": 17, \"heureux\": 18, \"triste\": 19,\n",
    "    \"grand\": 20, \"petit\": 21, \"rapide\": 22, \"lent\": 23, \"gar√ßon\": 24, \"fille\": 25,\n",
    "    \"joue\": 26, \"lit\": 27, \"√©crit\": 28, \"voiture\": 29, \"maison\": 30,\n",
    "    \"jouent\": 31, \"lisent\": 32, \"√©crivent\": 33, \"marchent\": 34, \"courent\": 35,\"est\":36,'sont': 37,\n",
    "    \"<pad>\": 0, \"<sos>\": 38, \"<eos>\": 39\n",
    "}\n",
    "\n",
    "# Reverse mapping for translation\n",
    "target_vocab_inv = {v: k for k, v in target_vocab_extended.items()}\n",
    "\n",
    "# Predefined sentence pairs (each word is in the vocab)\n",
    "sentence_pairs = [\n",
    "    (\"I love you\", \"Je t‚Äôaime\"),\n",
    "    (\"hello world\", \"bonjour monde\"),\n",
    "    (\"good morning\", \"bon matin\"),\n",
    "    (\"good night\", \"bon nuit\"),\n",
    "    (\"cat is small\", \"chat est petit\"),\n",
    "    (\"dog is big\", \"chien est grand\"),\n",
    "    (\"he runs fast\", \"il court rapide\"),\n",
    "    (\"she walks slow\", \"elle marche lent\"),\n",
    "    (\"we eat good\", \"nous mange bon\"),\n",
    "    (\"they are happy\", \"ils sont heureux\"),\n",
    "    (\"boy plays\", \"gar√ßon joue\"),\n",
    "    (\"girl reads\", \"fille lit\"),\n",
    "    (\"he writes\", \"il √©crit\"),\n",
    "    (\"car is big\", \"voiture est grand\"),\n",
    "    (\"house is small\", \"maison est petit\"),\n",
    "    (\"she is sad\", \"elle est triste\"),\n",
    "    (\"we go\", \"nous va\"),\n",
    "    (\"they run morning\", \"ils courent matin\"),\n",
    "    (\"I read\", \"Je lisent\"),\n",
    "    (\"dog walks fast\", \"chien marche rapide\")\n",
    "]\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize(sentence, vocab):\n",
    "    words = sentence.split()  # No lowercasing to preserve vocabulary matching\n",
    "    return [vocab[word] for word in words if word in vocab] + [vocab[\"<eos>\"]]\n",
    "\n",
    "# Generate tokenized dataset\n",
    "data_samples = []\n",
    "for src, tgt in sentence_pairs:\n",
    "    src_tokenized = tokenize(src, source_vocab_extended)\n",
    "    tgt_tokenized = [target_vocab_extended[\"<sos>\"]] + tokenize(tgt, target_vocab_extended)\n",
    "    data_samples.append((src_tokenized, tgt_tokenized))\n",
    "\n",
    "# Convert tokenized data into tensor format\n",
    "max_seq_length = max(max(len(src), len(tgt)) for src, tgt in data_samples)\n",
    "\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "src_data = torch.zeros((len(data_samples), max_seq_length), dtype=torch.long)\n",
    "tgt_data = torch.zeros((len(data_samples), max_seq_length), dtype=torch.long)\n",
    "\n",
    "for i, (src_seq, tgt_seq) in enumerate(data_samples):\n",
    "    src_data[i, :len(src_seq)] = torch.tensor(src_seq, dtype=torch.long)\n",
    "    tgt_data[i, :len(tgt_seq)] = torch.tensor(tgt_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d5050fcb-1461-465f-be15-ac3cab55c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Source: tensor([[11, 34, 39,  0,  0],\n",
      "        [30, 21, 39,  0,  0],\n",
      "        [10, 21, 39,  0,  0],\n",
      "        [14, 17,  7, 39,  0],\n",
      "        [ 1,  2,  3, 39,  0],\n",
      "        [10, 35, 23, 39,  0],\n",
      "        [14, 37, 19, 39,  0],\n",
      "        [13, 16,  6, 39,  0],\n",
      "        [12, 35, 24, 39,  0],\n",
      "        [11, 36, 23, 39,  0],\n",
      "        [31, 22, 39,  0,  0],\n",
      "        [ 4,  5, 39,  0,  0],\n",
      "        [12, 20, 39,  0,  0],\n",
      "        [25, 32, 39,  0,  0],\n",
      "        [26, 33, 39,  0,  0],\n",
      "        [ 1, 28, 39,  0,  0],\n",
      "        [13, 15, 39,  0,  0],\n",
      "        [ 6,  8, 39,  0,  0],\n",
      "        [ 6,  7, 39,  0,  0],\n",
      "        [ 9, 22, 39,  0,  0]])\n",
      "Batch Target: tensor([[38, 10, 28, 39,  0],\n",
      "        [38, 29, 36, 20, 39],\n",
      "        [38,  9, 36, 20, 39],\n",
      "        [38, 13, 35,  6, 39],\n",
      "        [38,  1,  2, 39,  0],\n",
      "        [38,  9, 17, 22, 39],\n",
      "        [38, 13, 37, 18, 39],\n",
      "        [38, 12, 15,  5, 39],\n",
      "        [38, 11, 17, 23, 39],\n",
      "        [38, 10, 16, 22, 39],\n",
      "        [38, 30, 36, 21, 39],\n",
      "        [38,  3,  4, 39,  0],\n",
      "        [38, 11, 36, 19, 39],\n",
      "        [38, 24, 26, 39,  0],\n",
      "        [38, 25, 27, 39,  0],\n",
      "        [38,  1, 32, 39,  0],\n",
      "        [38, 12, 14, 39,  0],\n",
      "        [38,  5,  7, 39,  0],\n",
      "        [38,  5,  6, 39,  0],\n",
      "        [38,  8, 36, 21, 39]])\n",
      "torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "dataset = TensorDataset(src_data, tgt_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Example usage: iterating through DataLoader\n",
    "for batch_src, batch_tgt in data_loader:\n",
    "    print(\"Batch Source:\", batch_src)\n",
    "    print(\"Batch Target:\", batch_tgt)\n",
    "    print(batch_src.shape)\n",
    "    break\n",
    "    \n",
    "    \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "60d75e30-299b-4b8b-b312-dc272e1dfd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f5e38a3f-a213-4ca2-99cb-e665371a3d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8768, -1.1023, -0.0734,  ..., -0.6387, -0.1363,  0.0523],\n",
      "         [-0.5000, -0.4919, -0.2958,  ..., -0.5241, -0.4329,  0.0198],\n",
      "         [ 0.0143, -0.2099,  1.1591,  ...,  0.8882, -0.5303, -0.7495],\n",
      "         [ 0.1193,  0.5671, -0.5095,  ..., -0.1803, -0.9110,  0.1656]],\n",
      "\n",
      "        [[-1.1052, -1.1976,  0.0801,  ...,  0.5014, -0.6975, -0.5310],\n",
      "         [-1.2910, -0.8343, -0.2651,  ...,  0.0698, -0.5597, -0.2602],\n",
      "         [ 0.3867,  0.5624,  0.4221,  ...,  1.0820, -0.3656, -0.8576],\n",
      "         [ 0.3392,  0.6169,  0.3211,  ...,  0.3042, -0.9341,  0.1040]],\n",
      "\n",
      "        [[-0.5663, -0.7348, -0.1208,  ..., -0.6185, -0.2072,  0.1170],\n",
      "         [-0.2922, -0.3419, -0.0221,  ..., -0.5045, -0.3769,  0.0067],\n",
      "         [-0.2445, -0.7056,  1.2907,  ..., -0.0483, -0.0653, -0.1523],\n",
      "         [ 0.4961,  0.7027, -0.0429,  ...,  0.2873, -0.7494, -0.1113]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1542, -1.2012, -0.0465,  ...,  0.1649, -0.4956, -0.4125],\n",
      "         [ 0.9605,  0.5651, -0.0315,  ..., -0.4023, -0.1956,  0.1835],\n",
      "         [-0.7904, -1.2679,  0.1830,  ...,  0.3499, -0.1001, -0.6072],\n",
      "         [-0.1160, -0.2048,  1.0913,  ...,  1.0639, -0.7555, -0.5892]],\n",
      "\n",
      "        [[-1.0004, -1.3400,  0.1844,  ...,  0.1808, -0.4492, -0.3987],\n",
      "         [ 0.1585, -0.2029,  0.1637,  ...,  0.6120, -0.4241, -0.5659],\n",
      "         [-0.2683,  0.1208,  0.0605,  ..., -0.2694, -0.2369,  0.0021],\n",
      "         [-0.1700, -0.1026,  0.4640,  ...,  1.7833, -0.8263, -1.0660]],\n",
      "\n",
      "        [[-0.6994, -0.8732, -0.2658,  ..., -0.5348, -0.1478,  0.0362],\n",
      "         [ 0.1015, -0.3365,  0.0236,  ..., -0.1905,  0.5511, -0.0314],\n",
      "         [-0.1750, -0.4115,  0.6119,  ..., -0.0504, -0.5912, -0.0134],\n",
      "         [-0.2591, -0.3148,  1.1187,  ...,  0.9532, -0.7749, -0.5698]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[13, 35,  6, 39],\n",
      "        [ 9, 17, 22, 39],\n",
      "        [13, 37, 18, 39],\n",
      "        [11, 17, 23, 39],\n",
      "        [12, 15,  5, 39],\n",
      "        [ 1, 32, 39,  0],\n",
      "        [ 9, 36, 20, 39],\n",
      "        [24, 26, 39,  0],\n",
      "        [12, 14, 39,  0],\n",
      "        [ 5,  6, 39,  0],\n",
      "        [11, 36, 19, 39],\n",
      "        [10, 28, 39,  0],\n",
      "        [ 3,  4, 39,  0],\n",
      "        [10, 16, 22, 39],\n",
      "        [29, 36, 20, 39],\n",
      "        [30, 36, 21, 39],\n",
      "        [ 1,  2, 39,  0],\n",
      "        [ 5,  7, 39,  0],\n",
      "        [ 8, 36, 21, 39],\n",
      "        [25, 27, 39,  0]])\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "transformer.train()\n",
    "for epoch in range(num_epochs):  # Train for 10 epochs\n",
    "    for batch_src, batch_tgt in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        output = transformer(batch_src, batch_tgt[:, :-1])\n",
    "        print(output)\n",
    "        print(batch_tgt[:, 1:])\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), batch_tgt[:, 1:].contiguous().view(-1))\n",
    "        # loss = criterion(output.view(-1, tgt_vocab_size), batch_tgt[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:  # Print every epoch\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a2da0b75-1fc4-4f12-83d6-f06f0d46d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(tokens, vocab_inv):\n",
    "    return \" \".join([vocab_inv[token] for token in tokens if token in vocab_inv and token not in [0, 38, 39]])\n",
    "                    \n",
    "# Function for model inference\n",
    "def translate_sentence(model, sentence, src_vocab, tgt_vocab_inv, max_length=max_seq_length):\n",
    "    model.eval()\n",
    "    src_tokens = tokenize(sentence, src_vocab)\n",
    "    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    tgt_tokens = [38]  # Start with <sos>\n",
    "    for _ in range(max_length):\n",
    "        tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0)\n",
    "        output = model(src_tensor, tgt_tensor)\n",
    "        next_token = output.argmax(-1)[:, -1].item()\n",
    "        if next_token == 39:  # If <eos> token is generated, stop\n",
    "            break\n",
    "        tgt_tokens.append(next_token)\n",
    "    \n",
    "    return decode_tokens(tgt_tokens, tgt_vocab_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7998987-95b2-4ca5-a6fe-8afabaf11bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs = [\n",
    "    (\"I love you\", \"Je t‚Äôaime\"),\n",
    "    (\"hello world\", \"bonjour monde\"),\n",
    "    (\"good morning\", \"bon matin\"),\n",
    "    (\"good night\", \"bon nuit\"),\n",
    "    (\"cat is small\", \"chat est petit\"),\n",
    "    (\"dog is big\", \"chien est grand\"),\n",
    "    (\"he runs fast\", \"il court rapide\"),\n",
    "    (\"she walks slow\", \"elle marche lent\"),\n",
    "    (\"we eat good\", \"nous mange bon\"),\n",
    "    (\"they are happy\", \"ils sont heureux\"),\n",
    "    (\"boy plays\", \"gar√ßon joue\"),\n",
    "    (\"girl reads\", \"fille lit\"),\n",
    "    (\"he writes\", \"il √©crit\"),\n",
    "    (\"car is big\", \"voiture est grand\"),\n",
    "    (\"house is small\", \"maison est petit\"),\n",
    "    (\"she is sad\", \"elle est triste\"),\n",
    "    (\"we go\", \"nous va\"),\n",
    "    (\"they run morning\", \"ils courent matin\"),\n",
    "    (\"I read\", \"Je lisent\"),\n",
    "    (\"dog walks fast\", \"chien marche rapide\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0807f1b4-5656-4ce3-b95a-d332007bcf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "üîπ Input: I love you\n",
      "‚úÖ Expected Translation: Je t‚Äôaime\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: hello world\n",
      "‚úÖ Expected Translation: bonjour monde\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: good morning\n",
      "‚úÖ Expected Translation: bon matin\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: good night\n",
      "‚úÖ Expected Translation: bon nuit\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: cat is small\n",
      "‚úÖ Expected Translation: chat est petit\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: dog is big\n",
      "‚úÖ Expected Translation: chien est grand\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "üîπ Input: he runs fast\n",
      "‚úÖ Expected Translation: il court rapide\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "üîπ Input: she walks slow\n",
      "‚úÖ Expected Translation: elle marche lent\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "üîπ Input: we eat good\n",
      "‚úÖ Expected Translation: nous mange bon\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "üîπ Input: they are happy\n",
      "‚úÖ Expected Translation: ils sont heureux\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: boy plays\n",
      "‚úÖ Expected Translation: gar√ßon joue\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: girl reads\n",
      "‚úÖ Expected Translation: fille lit\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: he writes\n",
      "‚úÖ Expected Translation: il √©crit\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: car is big\n",
      "‚úÖ Expected Translation: voiture est grand\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: house is small\n",
      "‚úÖ Expected Translation: maison est petit\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: she is sad\n",
      "‚úÖ Expected Translation: elle est triste\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: we go\n",
      "‚úÖ Expected Translation: nous va\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "üîπ Input: they run morning\n",
      "‚úÖ Expected Translation: ils courent matin\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "encoderlayer_d: torch.Size([1, 3, 128])\n",
      "üîπ Input: I read\n",
      "‚úÖ Expected Translation: Je lisent\n",
      "üîç Predicted Translation: \n",
      "\n",
      "encoderlayer_d: torch.Size([1, 4, 128])\n",
      "üîπ Input: dog walks fast\n",
      "‚úÖ Expected Translation: chien marche rapide\n",
      "üîç Predicted Translation: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for english_sentence, expected_french in sentence_pairs:\n",
    "    predicted_translation = translate_sentence(transformer, english_sentence, source_vocab_extended, target_vocab_inv)\n",
    "    \n",
    "    print(f\"üîπ Input: {english_sentence}\")\n",
    "    print(f\"‚úÖ Expected Translation: {expected_french}\")\n",
    "    print(f\"üîç Predicted Translation: {predicted_translation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a657aa-432b-4209-9101-5e1307e83f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96988a01-8369-4014-b7d3-52ed2e5ddcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d31105-f132-4a93-bc0b-ded64cf52403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
